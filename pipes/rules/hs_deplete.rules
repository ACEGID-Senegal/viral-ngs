"""
    This is a basic framework for depleting human and other contaminant 
    reads from NGS data.  All non-human reads should remain behind.
"""

__author__ = 'Kristian Andersen <andersen@broadinstitute.org>, Daniel Park <dpark@broadinstitute.org>'

from snakemake.utils import makedirs
import os

rule all_deplete:
    input:
        expand("{dataDir}/{subdir}/{sample}.{adjective}.bam",
            dataDir=config["dataDir"],
            subdir=config["subdirs"]["depletion"],
            adjective=['raw','cleaned'],
            sample=read_samples_file(config["samples_depletion"])),
    params: LSF="-N"

rule revert_bam:
    input:  config["dataDir"]+'/'+config["subdirs"]["source"]+'/{sample}.bam'
    output: config["dataDir"]+'/'+config["subdirs"]["depletion"]+'/{sample}.raw.bam'
    resources: mem=3
    params: LSF='-W 4:00 -sp 60',
            logid="{sample}"
    run:
            makedirs(expand("{dir}/{subdir}",
                dir=[config["dataDir"],config["tmpDir"]],
                subdir=config["subdirs"]["depletion"]))
            shell("{config[binDir]}/read_utils.py revert_bam_picard {input} {output} --picardOptions SORT_ORDER=queryname SANITIZE=true")

rule deplete_bmtagger:
    ''' This step depletes human reads using BMTagger
    '''
    input:  config["dataDir"]+'/'+config["subdirs"]["depletion"]+'/{sample}.raw.bam'
    #       expand("{dbdir}/{db}.{suffix}",
    #           dbdir=config["bmTaggerDbDir"],
    #           db=config["bmTaggerDbs_remove"],
    #           suffix=["bitmask","srprism.idx","srprism.map"])
    output: config["tmpDir"]+ '/'+config["subdirs"]["depletion"]+'/{sample}.bmtagger_depleted.bam'
    resources: mem=10
    params: LSF='-q forest',
            refDbs = expand("{dbdir}/{db}", dbdir=config["bmTaggerDbDir"], db=config["bmTaggerDbs_remove"]),
            logid="{sample}"
    shell:  "{config[binDir]}/taxon_filter.py deplete_bam_bmtagger {input} {params.refDbs} {output}"

rule rmdup_mvicuna:
    ''' This step removes PCR duplicate reads using M-Vicuna (our custom
        tool included here).  
        Unlike Picard MarkDuplicates, M-Vicuna does not require reads to be
        previously aligned.
        Note that this requires that we see all reads for a given library
        at once, so we have to re-merge all of the split files, and then
        split them out again when we're done.
    '''
    input:  config["tmpDir"]+ '/'+config["subdirs"]["depletion"]+'/{sample}.bmtagger_depleted.bam'
    output: config["tmpDir"]+ '/'+config["subdirs"]["depletion"]+'/{sample}.rmdup.bam'
    resources: mem=7
    params: LSF='-q forest',
            logid="{sample}"
    shell:  "{config[binDir]}/read_utils.py rmdup_mvicuna_bam {input} {output}"

rule deplete_blastn:
    ''' This step depletes human reads using BLASTN, which is more sensitive,
        but much slower than BMTagger, so we run it last.
    '''
    input:  config["tmpDir"] +'/'+config["subdirs"]["depletion"]+'/{sample}.rmdup.bam'
    output: config["dataDir"]+'/'+config["subdirs"]["depletion"]+'/{sample}.cleaned.bam'
    resources: mem=5
    params: LSF='-W 4:00',
            refDbs = expand("{dbdir}/{db}", dbdir=config["blastDbDir"], db=config["blastDb_remove"]),
            logid="{sample}"
    shell:  "{config[binDir]}/taxon_filter.py deplete_blastn_bam {input} {params.refDbs} {output}"

rule filter_to_taxon:
    ''' This step reduces the read set to a specific taxon (usually the genus
        level or greater for the virus of interest).
    '''
    input:  config["dataDir"]+'/'+config["subdirs"]["depletion"]+'/{sample}.cleaned.bam'
    output: config["dataDir"]+'/'+config["subdirs"]["depletion"]+'/{sample}.taxfilt.bam'
    resources: mem=8
    params: LSF='-W 4:00',
            refDb=config["lastal_refDb"],
            logid="{sample}"
    shell:  "{config[binDir]}/taxon_filter.py filter_lastal_bam {input} {params.refDb} {output}"


def merge_one_per_sample_inputs(wildcards):
    if 'seqruns_demux' not in config or not os.path.isfile(config['seqruns_demux']):
        return config["dataDir"]+'/'+config["subdirs"]["depletion"] +'/'+ wildcards.sample + '.' + wildcards.adjective + '.bam'
    runs = set()
    for flowcell in read_samples_file(config['seqruns_demux']):
        for lane in read_tab_file(flowcell):
            for well in read_tab_file(lane['barcode_file']):
                if well['sample'] == wildcards.sample:
                    libs.add(os.path.join(config["dataDir"], config["subdirs"]["depletion"],
                        get_run_id(well) +'.'+ lane['flowcell'] +'.'+ lane['lane'] +'.'+ wildcards.adjective + '.bam'))
    return sorted(runs)
rule merge_one_per_sample:
    input:  merge_one_per_sample_inputs
    output: config["dataDir"]+'/'+config["subdirs"]["per_sample"] +'/{sample}.{adjective,raw|cleaned|taxfilt}.bam'
    resources: mem=8
    params: LSF='-q forest',
            logid="{sample}",
            tmpf_bam=config["tmpDir"]+'/'+config["subdirs"]["depletion"] +'/{sample}.{adjective}.bam'
    run:
            makedirs(config["dataDir"]+'/'+config["subdirs"]["per_sample"])
            if wildcards.adjective == 'raw':
                shell("{config[binDir]}/read_utils.py merge_bams {input} {output} --picardOptions SORT_ORDER=queryname")
            else:
                shell("{config[binDir]}/read_utils.py merge_bams {input} {params.tmpf_bam} --picardOptions SORT_ORDER=queryname")
                shell("{config[binDir]}/read_utils.py rmdup_mvicuna_bam {params.tmpf_bam} {output}")

