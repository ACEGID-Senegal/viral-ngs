"""
    This is a basic framework for depleting human and other contaminant 
    reads from NGS data.  All non-human reads should remain behind.
"""

__author__ = 'Kristian Andersen <andersen@broadinstitute.org>, Daniel Park <dpark@broadinstitute.org>'

from snakemake.utils import makedirs
import os

rule all_deplete:
    input:
        expand("{dataDir}/{subdir}/{sample}.{adjective}.bam",
            dataDir=config["dataDir"],
            subdir=config["subdirs"]["depletion"],
            adjective=['raw','cleaned'],
            sample=read_samples_file(config["samples_depletion"])),
    params: LSF="-N"

rule revert_bam:
    input:  config["dataDir"]+'/'+config["subdirs"]["source"]+'/{sample}.bam'
    output: config["dataDir"]+'/'+config["subdirs"]["depletion"]+'/{sample}.raw.bam'
    resources: mem=3
    params: LSF='-W 4:00 -sp 60',
            logid="{sample}"
    run:
            makedirs(expand("{dir}/{subdir}",
                dir=[config["dataDir"],config["tmpDir"]],
                subdir=config["subdirs"]["depletion"]))
            shell("{config[binDir]}/read_utils.py revert_bam_picard {input} {output} --picardOptions SORT_ORDER=queryname SANITIZE=true")

"""
rule split_bam:
    ''' As many of the depletion steps are computationally slow, we split
        and remerge the BAMs at various points.
    '''
    input:  config["dataDir"]+'/'+config["subdirs"]["depletion"]+'/{sample}.raw.bam'
    output: expand('{dir}/{{sample}}.raw.{split_id:03d}.bam', \
                dir = config["tmpDir"] + '/' + config["subdirs"]["depletion"], \
                split_id = range(1,1+config["deplete_bmtagger_nchunks"]))
    resources: mem=3
    params: LSF='-W 4:00',
            logid="{sample}"
    shell:  "{config[binDir]}/read_utils.py split_bam {input} {output}"

rule deplete_bmtagger_split:
    ''' This step depletes human reads using BMTagger
    '''
    input:  '{dir}/{sample}.raw.{split_id}.bam'
    #       expand("{dbdir}/{db}.{suffix}",
    #           dbdir=config["bmTaggerDbDir"],
    #           db=config["bmTaggerDbs_remove"],
    #           suffix=["bitmask","srprism.idx","srprism.map"])
    output: '{dir}/{sample}.bmtagger_depleted.{split_id,\d+}.bam'
    resources: mem=9
    params: LSF='-W 4:00 -sp 40',
            refDbs = expand("{dbdir}/{db}", dbdir=config["bmTaggerDbDir"], db=config["bmTaggerDbs_remove"]),
            logid="{sample}-{split_id}"
    shell:  "{config[binDir]}/taxon_filter.py deplete_bam_bmtagger {input} {params.refDbs} {output}"

rule rmdup_mvicuna_split:
    ''' This step removes PCR duplicate reads using M-Vicuna (our custom
        tool included here).  
        Unlike Picard MarkDuplicates, M-Vicuna does not require reads to be
        previously aligned.
        Note that this requires that we see all reads for a given library
        at once, so we have to re-merge all of the split files, and then
        split them out again when we're done.
    '''
    input:
            expand('{{dir}}/{{sample}}.bmtagger_depleted.{split_id:03d}.bam',
                split_id = range(1,1+config["deplete_bmtagger_nchunks"]))
    output: 
            expand('{{dir}}/{{sample}}.rmdup.{split_id:03d}.bam',
                split_id = range(1,1+config["deplete_blast_nchunks"]))
    resources: mem=7
    params:
            LSF='-q forest -sp 75',
            logid="{sample}",
            tmpf_input = '{dir}/{sample}.bmtagger_depleted.bam',
            tmpf_output = '{dir}/{sample}.rmdup.bam'
    run:
            shell("{config[binDir]}/read_utils.py merge_bams {input} {params.tmpf_input} --picardOptions SORT_ORDER=queryname")
            shell("{config[binDir]}/read_utils.py rmdup_mvicuna_bam {params.tmpf_input} {params.tmpf_output}")
            shell("{config[binDir]}/read_utils.py split_bam {params.tmpf_output} {output}")
            os.unlink(params.tmpf_input)
            os.unlink(params.tmpf_output)

rule deplete_blastn_split:
    ''' This step depletes human reads using BLASTN, which is more sensitive,
        but much slower than BMTagger, so we run it last.
    '''
    input:  '{dir}/{sample}.rmdup.{split_id}.bam'
    output: '{dir}/{sample}.cleaned.{split_id,\d+}.bam'
    resources: mem=4
    params: LSF='-W 4:00 -sp 40',
            refDbs = expand("{dbdir}/{db}", dbdir=config["blastDbDir"], db=config["blastDb_remove"]),
            logid="{sample}-{split_id}"
    shell:  "{config[binDir]}/taxon_filter.py deplete_blastn_bam {input} {params.refDbs} {output}"

rule merge_to_clean_bam:
    input:
            expand('{dir}/{{sample}}.cleaned.{split_id:03d}.bam',
                dir = config["tmpDir"]+'/'+config["subdirs"]["depletion"],
                split_id = range(1,1+config["deplete_blast_nchunks"]))
    output: config["dataDir"]+'/'+config["subdirs"]["depletion"]+'/{sample}.cleaned.bam'
    resources: mem=3
    params: logid="{sample}"
    shell:  "{config[binDir]}/read_utils.py merge_bams {input} {output} --picardOptions SORT_ORDER=queryname"
"""

rule deplete_bmtagger:
    ''' This step depletes human reads using BMTagger
    '''
    input:  config["dataDir"]+'/'+config["subdirs"]["depletion"]+'/{sample}.raw.bam'
    #       expand("{dbdir}/{db}.{suffix}",
    #           dbdir=config["bmTaggerDbDir"],
    #           db=config["bmTaggerDbs_remove"],
    #           suffix=["bitmask","srprism.idx","srprism.map"])
    output: config["tmpDir"]+ '/'+config["subdirs"]["depletion"]+'/{sample}.bmtagger_depleted.bam'
    resources: mem=10
    params: LSF='-q forest',
            refDbs = expand("{dbdir}/{db}", dbdir=config["bmTaggerDbDir"], db=config["bmTaggerDbs_remove"]),
            logid="{sample}"
    shell:  "{config[binDir]}/taxon_filter.py deplete_bam_bmtagger {input} {params.refDbs} {output}"

rule rmdup_mvicuna:
    ''' This step removes PCR duplicate reads using M-Vicuna (our custom
        tool included here).  
        Unlike Picard MarkDuplicates, M-Vicuna does not require reads to be
        previously aligned.
        Note that this requires that we see all reads for a given library
        at once, so we have to re-merge all of the split files, and then
        split them out again when we're done.
    '''
    input:  config["tmpDir"]+ '/'+config["subdirs"]["depletion"]+'/{sample}.bmtagger_depleted.bam'
    output: config["tmpDir"]+ '/'+config["subdirs"]["depletion"]+'/{sample}.rmdup.bam'
    resources: mem=7
    params: LSF='-q forest',
            logid="{sample}"
    shell:  "{config[binDir]}/read_utils.py rmdup_mvicuna_bam {input} {output}"

rule deplete_blastn:
    ''' This step depletes human reads using BLASTN, which is more sensitive,
        but much slower than BMTagger, so we run it last.
    '''
    input:  config["tmpDir"] +'/'+config["subdirs"]["depletion"]+'/{sample}.rmdup.bam'
    output: config["dataDir"]+'/'+config["subdirs"]["depletion"]+'/{sample}.cleaned.bam'
    resources: mem=4
    params: LSF='-W 4:00',
            refDbs = expand("{dbdir}/{db}", dbdir=config["blastDbDir"], db=config["blastDb_remove"]),
            logid="{sample}"
    shell:  "{config[binDir]}/taxon_filter.py deplete_blastn_bam {input} {params.refDbs} {output}"
